{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de02150-a926-4dd2-bf82-61b8af37ac12",
   "metadata": {},
   "source": [
    "# ```generate()``` and ü§óaccelerate\n",
    "This notebook is for people using ```model.generate()``` with multiple GPUs. I ‚ù§Ô∏è ü§ó accelerate but some things were not obvious to me. \n",
    "\n",
    "I wish I had these examples earlier when starting out. In my opinion, the most important functions to know are `split_between_processes` and `gather_object`.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7097c-a784-4e23-acd7-c7ff98fd509b",
   "metadata": {},
   "source": [
    "## 0. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0339dbf-d954-41ee-8982-56ca77c76f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "DistributedType.MULTI_GPU\n",
      "DistributedType.MULTI_GPU\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "def test():\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    print(accelerator.distributed_type)\n",
    "\n",
    "notebook_launcher(test, num_processes=2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f5354-d33d-4943-b272-7eb44a852975",
   "metadata": {},
   "source": [
    "## 1. Hello world\n",
    "simplest example: create strings on each GPU and collect them using ```gather_object()```\n",
    "\n",
    "change ```num_processes``` to the number of GPUs in your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e912559d-7ebb-4e92-9b3e-c053d7c6d834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 5 GPUs.\n",
      "['Hello this is GPU 0', 'Hello this is GPU 1', 'Hello this is GPU 2', 'Hello this is GPU 3', 'Hello this is GPU 4']\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "def hello_world():\n",
    "    from accelerate import Accelerator\n",
    "    from accelerate.utils import gather_object\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    message= [f\"Hello this is GPU {accelerator.process_index}\"]\n",
    "    messages=gather_object(message)\n",
    "\n",
    "    accelerator.print(messages)\n",
    "\n",
    "notebook_launcher(hello_world, num_processes=5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0dce0-b22f-4e56-b598-b2dea9d2e657",
   "metadata": {},
   "source": [
    "## 2. Multi-GPU text-generation\n",
    "* load a model on each GPU\n",
    "* distribute the prompts with ```split_between_processes```\n",
    "* have each GPU ```generate```\n",
    "* gather and output the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c347842a-2e54-4bd0-832b-e74d7493f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 5 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> The King is dead. Long live the Queen.\n",
      "The King is dead. Long live the Queen.\n",
      "The King is dead. Long live the King.\n",
      "The King is dead. Long live the King.\n",
      "The King is dead. Long live the King.\n",
      "The King is dead.\n",
      "<s> Once there were four children whose names were Peter, Susan, Edmund, and Lucy. This story is about Lucy.\n",
      "Lucy was the youngest of the four children. She was eleven years old, and she was very small for her age. She was also very curious.\n",
      "One day, Lucy was playing in the garden with her brother\n",
      "<s> The story so far: in the beginning, the universe was created. This has made a lot of people very angry and been widely regarded as a bad move.\n",
      "The story so far: in the beginning, the universe was created. This has made a lot of people very angry and been widely regarded as a bad move. Then God said, \"Let there be\n",
      "<s> It was a queer, sultry summer, the summer they electrocuted the Rosenbergs, and I didn‚Äôt know what I was doing in New York. I was 23 years old, my wife was pregnant, and though we were living in a nice apartment on the Upper West Side, our finances were in dire straits. We had two\n",
      "<s> We were somewhere around Barstow on the edge of the desert when the drugs began to take hold.\n",
      "I remember saying something like, \"I feel a bit lightheaded; maybe you should drive...\"\n",
      "And suddenly there was a terrible roar all around us and the sky was full of what looked like huge bats, all swooping\n",
      "<s> You better not never tell nobody but God. It‚Äôd kill your mammy.\n",
      "I‚Äôm not saying I‚Äôm better than you. I‚Äôm just saying I‚Äôm not like you.\n",
      "I‚Äôm not saying I‚Äôm better than you. I‚Äôm just saying I‚Äôm not like you.\n",
      "<s> It was a bright cold day in April, and the clocks were striking thirteen.\n",
      "Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of grit\n",
      "<s> It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\n",
      "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this\n",
      "<s> The snow in the mountains was melting and Bunny had been dead for several weeks before we came to understand the gravity of our situation.\n",
      "The first sign of trouble was the smell.\n",
      "It was a smell that had been with us for some time, but we had been too busy to notice it. It was a smell that had been with us for\n",
      "<s> The sweat wis lashing oafay Sick Boy; he wis trembling.\n",
      "\"I'm no' gonnae tell ye,\" he said. \"I'm no' gonnae tell ye.\"\n",
      "\"I'm no' gonnae tell ye,\" he said. \"I'm no'\n",
      "<s> 124 was spiteful. Full of Baby's venom.\n",
      "124 was spiteful. Full of Baby's venom.\n",
      "124 was spiteful. Full of Baby's venom. 124 was spiteful. Full of Baby's venom. 1\n",
      "<s> As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect. He was laying on his back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections. The bedding was hardly able to cover it and seemed ready to\n",
      "<s> When Mary Lennox was sent to Misselthwaite Manor to live with her uncle everybody said she was the most disagreeable-looking child ever seen. She was thin and freckled, had an imperious manner, and was never known to smile.\n",
      "The manor house stood high on the moors, and in every direction there were\n",
      "<s> I write this sitting in the kitchen sink. I‚Äôm not sure why I‚Äôm here. I‚Äôm not sure why I‚Äôm writing this. I‚Äôm not sure why I‚Äôm writing this in the kitchen sink. I‚Äôm not sure why I‚Äôm writing this\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "def hello_world():\n",
    "    from accelerate import Accelerator\n",
    "    from accelerate.utils import gather_object\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import torch\n",
    "    \n",
    "    # https://www.penguin.co.uk/articles/2022/04/best-first-lines-in-books\n",
    "    prompts_all=[\n",
    "        \"The King is dead. Long live the Queen.\",\n",
    "        \"Once there were four children whose names were Peter, Susan, Edmund, and Lucy. This story\",\n",
    "        \"The story so far: in the beginning, the universe was created. This has made a lot of people very angry\",\n",
    "        \"It was a queer, sultry summer, the summer they electrocuted the Rosenbergs, and I didn‚Äôt know what\",\n",
    "        \"We were somewhere around Barstow on the edge of the desert when the drugs began to take hold.\",\n",
    "        \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
    "        \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",\n",
    "        \"The snow in the mountains was melting and Bunny had been dead for several weeks before we came to understand the gravity of\",\n",
    "        \"The sweat wis lashing oafay Sick Boy; he wis trembling.\",\n",
    "        \"124 was spiteful. Full of Baby's venom.\",\n",
    "        \"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.\",\n",
    "        \"When Mary Lennox was sent to Misselthwaite Manor to live with her uncle everybody said she was\",\n",
    "        \"I write this sitting in the kitchen sink.\",\n",
    "    ]\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    model_path=\"models/llama2-7b\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,    \n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)   \n",
    "\n",
    "    with accelerator.split_between_processes(prompts_all) as prompts:\n",
    "        outputs=[]\n",
    "        for prompt in prompts:\n",
    "            prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=50)\n",
    "            outputs.append(tokenizer.decode(output_tokenized[0]))\n",
    "\n",
    "    outputs_gathered=gather_object(outputs)\n",
    "\n",
    "    for output in outputs_gathered:\n",
    "        accelerator.print(output)\n",
    "\n",
    "    with open('outputs.txt','w') as file:\n",
    "        file.write('\\n\\n'.join(outputs_gathered))\n",
    "\n",
    "notebook_launcher(hello_world, num_processes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fa3785-6a2d-4fa0-b214-9880936cd84b",
   "metadata": {},
   "source": [
    "## 3. Generate while training Multi-GPUs\n",
    "This one adds a bit more, here we train a tiny LLM and see how it's output changes during training. These are the steps:\n",
    "* load ```Locutusque/TinyMistral-248M```\n",
    "* load ```timdettmers/openassistant-guanaco```\n",
    "* make up a few random ```prompts```\n",
    "* add a ```TrainerCallback``` to evaluate after each epoch\n",
    "    * distribute ```prompts``` among the GPUs\n",
    "    * on each GPU: split the received prompts into batches (```bs=8``` in the code below)\n",
    "    * batched inference with ```generate()```\n",
    "    * collect outputs using ```gather_object```\n",
    "    * log, print, whatever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61d78d42-bde8-458f-a4d7-7643106d2eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 5 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-ronimo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/g/jupyter/wandb/run-20231122_100844-jy3zy2qa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-ronimo/huggingface/runs/jy3zy2qa' target=\"_blank\">true-cloud-250</a></strong> to <a href='https://wandb.ai/g-ronimo/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-ronimo/huggingface' target=\"_blank\">https://wandb.ai/g-ronimo/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-ronimo/huggingface/runs/jy3zy2qa' target=\"_blank\">https://wandb.ai/g-ronimo/huggingface/runs/jy3zy2qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [496/496 05:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.748200</td>\n",
       "      <td>3.525146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.814300</td>\n",
       "      <td>3.376099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.194700</td>\n",
       "      <td>3.405650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>3.433367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [496/496 05:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.748200</td>\n",
       "      <td>3.525146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.814300</td>\n",
       "      <td>3.376099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.194700</td>\n",
       "      <td>3.405650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>3.433367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [496/496 05:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.748200</td>\n",
       "      <td>3.525146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.814300</td>\n",
       "      <td>3.376099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.194700</td>\n",
       "      <td>3.405650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>3.433367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [496/496 05:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.748200</td>\n",
       "      <td>3.525146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.814300</td>\n",
       "      <td>3.376099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.194700</td>\n",
       "      <td>3.405650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>3.433367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [496/496 05:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.748200</td>\n",
       "      <td>3.525146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.814300</td>\n",
       "      <td>3.376099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.194700</td>\n",
       "      <td>3.405650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>3.433367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1.00:\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: Hello! I am Open Assistant, a chat-based chat-based chat-based chat-based chat-based chat-based chat-based chat\n",
      "### Human: How much is 2+2? Think step by step\n",
      "### Assistant: 1+2 = 1\n",
      "\n",
      "The average height of a sphere depends on the size of the Earth and the distance between the Earth and the\n",
      "### Human: What is on your mind?\n",
      "### Assistant: I am a computer program that is designed to provide a high-quality, high-quality, high-quality, high-quality, high-quality\n",
      "### Human: Define artificial general intelligence\n",
      "### Assistant: La idea de un concepto de concepto de concepto de concepto de concepto de concepto de concepto de concepto de concepto\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: Hello! I am Open Assistant, a chat-based chat-based chat-based chat-based chat-based chat-based chat-based chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2.00:\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: I am Open Assistant, a chat based chat based chat based on the open source chat. I am a chat based chat based on the open source chat\n",
      "### Human: How much is 2+2? Think step by step\n",
      "### Assistant: 3+2 = 2\n",
      "\n",
      "The average height in a 2 is 2.\n",
      "\n",
      "The average height in a 2 is\n",
      "### Human: What is on your mind?\n",
      "### Assistant: I am a language model, and I am trained on a large corpus of text from a large corpus of text from a large corpus of\n",
      "### Human: Define artificial general intelligence\n",
      "### Assistant: La capacidad de la inteligencia artificial (IA) es un concepto fundamental en la que se utiliza para entender c√≥mo se util\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: I am Open Assistant, a chat based chat based chat based on the open source chat. I am a chat based chat based on the open source chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3.00:\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: I am an open source AI language model trained by humans. I am trained on a large corpus of text data, which is generated by a large\n",
      "### Human: How much is 2+2? Think step by step\n",
      "### Assistant: 3 + 2 = 3\n",
      "\n",
      "There are 100 000 000 0000 \n",
      "### Human: What is on your mind?\n",
      "### Assistant: I am an AI language model,\n",
      "\n",
      "I am designed to answer questions and provide helpful answers to your questions.\n",
      "I am designed to respond to\n",
      "### Human: Define artificial general intelligence\n",
      "### Assistant: El concepto de artificial intelligence (AI) es una pregunta que ha sido debatida en muchas empresas, pero con much\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: I am an open source AI language model trained by humans. I am trained on a large corpus of text data, which is generated by a large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4.00:\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: Hello! I am Open Assistant, a chat based assistant that can answer questions, help with tasks, and more.### Human: I'm looking\n",
      "### Human: How much is 2+2? Think step by step\n",
      "### Assistant: 2+2 is the average size of the average person. \n",
      "\n",
      "However, this is just a rough estimate and can vary depending on factors\n",
      "### Human: What is on your mind?\n",
      "### Assistant: I am not a sentient AI and I do not have the ability to experience emotions. \n",
      "\n",
      "I do not have the ability to experience emotions\n",
      "### Human: Define artificial general intelligence\n",
      "### Assistant: Human: Hola, soy un modelo de lenguaje entrenado por la comunidad. ¬øEn qu√© puedo ayudarte\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: Hello! I am Open Assistant, a chat based assistant that can answer questions, help with tasks, and more.### Human: I'm looking\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "def hello_world():\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, TrainingArguments, Trainer, TrainerControl, TrainerCallback, TrainerState\n",
    "    from accelerate import Accelerator\n",
    "    from accelerate.utils import gather_object\n",
    "    from datasets import load_dataset\n",
    "    import torch\n",
    "    import os\n",
    "    \n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model_path=\"Locutusque/TinyMistral-248M\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,    \n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)   \n",
    "    \n",
    "    # Load and tokenize dataset\n",
    "    dataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n",
    "    \n",
    "    def tokenize(element):\n",
    "        return tokenizer(element[\"text\"], truncation=True, max_length=512, add_special_tokens=False)\n",
    "    dataset_tokenized = dataset.map(tokenize, batched=True, num_proc=os.cpu_count(), remove_columns=[\"text\"])\n",
    "    \n",
    "    # collate function - to transform list of dictionaries [ {input_ids: [123, ..]}, {.. ] to single batch dictionary { input_ids: [..], labels: [..], attention_mask: [..] }\n",
    "    def collate(elements):\n",
    "        tokenlist=[e[\"input_ids\"] for e in elements]\n",
    "        tokens_maxlen=max([len(t) for t in tokenlist])\n",
    "    \n",
    "        input_ids,labels,attention_masks = [],[],[]\n",
    "        for tokens in tokenlist:\n",
    "            pad_len=tokens_maxlen-len(tokens)\n",
    "    \n",
    "            # pad input_ids with pad_token, labels with ignore_index (-100) and set attention_mask 1 where content otherwise 0\n",
    "            input_ids.append( tokens + [tokenizer.pad_token_id]*pad_len )   \n",
    "            labels.append( tokens + [-100]*pad_len )    \n",
    "            attention_masks.append( [1]*len(tokens) + [0]*pad_len ) \n",
    "    \n",
    "        batch={\n",
    "            \"input_ids\": torch.tensor(input_ids),\n",
    "            \"labels\": torch.tensor(labels),\n",
    "            \"attention_mask\": torch.tensor(attention_masks)\n",
    "        }\n",
    "        return batch\n",
    "    \n",
    "    # List of prompts for evaluation\n",
    "    prompt_template=\"### Human: {}\\n### Assistant:\"\n",
    "    questions = [ \n",
    "        \"Hello! Who are you? Introduce yourself please\",\n",
    "        \"How much is 2+2? Think step by step\",\n",
    "        \"What is on your mind?\",\n",
    "        \"Define artificial general intelligence\",\n",
    "        ] * 10 # expand. not creative enough for more\n",
    "    prompts = [ prompt_template.format(q) for q in questions ]\n",
    "    \n",
    "    # Callback class for generation during training\n",
    "    class GenerateEvalCallback(TrainerCallback):\n",
    "        def __init__(self, prompts, accelerator):\n",
    "            self.prompts_all=prompts\n",
    "            self.accelerator=Accelerator()\n",
    "        \n",
    "        # left pad for inference and tokenize\n",
    "        def prepare_prompts(self, prompts, tokenizer):\n",
    "            tokenizer.padding_side=\"left\"     \n",
    "            prompts_tok=tokenizer(\n",
    "                prompts, \n",
    "                return_tensors=\"pt\", \n",
    "                padding='longest', \n",
    "                truncation=False, \n",
    "                pad_to_multiple_of=8,\n",
    "                add_special_tokens=False).to(\"cuda\")\n",
    "            tokenizer.padding_side=\"right\"\n",
    "    \n",
    "            return prompts_tok\n",
    "    \n",
    "        def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, model, tokenizer, eval_dataloader, **kwargs):\n",
    "            model.eval()\n",
    "            model.config.use_cache = True\n",
    "    \n",
    "            # split questions among GPUs\n",
    "            with accelerator.split_between_processes(self.prompts_all) as prompts:\n",
    "                # batched inference on each GPU\n",
    "                bs=8\n",
    "                batches=[prompts[i:i + bs] for i in range(0, len(prompts), bs)]  \n",
    "                outputs=[]   # outputs per GPU\n",
    "                for prompt_batch in batches:\n",
    "                    prompts_tok=self.prepare_prompts(prompt_batch, tokenizer)\n",
    "                    with torch.no_grad():\n",
    "                        outputs_tok=model.generate(**prompts_tok, max_new_tokens=30).to(\"cpu\")\n",
    "                    outputs.extend([\n",
    "                        tokenizer.decode(outputs_tok[i][outputs_tok[i]!=tokenizer.pad_token_id])\n",
    "                        for i,t in enumerate(outputs_tok) \n",
    "                        ])\n",
    "            outputs_gathered=gather_object(outputs)  # collect results from all GPUs\n",
    "\n",
    "            # print a few to console\n",
    "            accelerator.print(f\"EPOCH {state.epoch:0.2f}:\")\n",
    "            for output in outputs_gathered[:5]:  \n",
    "                accelerator.print(output)\n",
    "\n",
    "            # write all to file\n",
    "            if accelerator.is_main_process:\n",
    "                with open(f\"outputs_epoch-{state.epoch:0.2f}.txt\",'w') as file:\n",
    "                    file.write('\\n\\n'.join(outputs_gathered))\n",
    "    \n",
    "            model.config.use_cache = False\n",
    "            return control\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"out\",\n",
    "        per_device_train_batch_size=16,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_steps=1,\n",
    "        num_train_epochs=4,\n",
    "        learning_rate=0.001,\n",
    "        ddp_find_unused_parameters=False,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        data_collator=collate,\n",
    "        train_dataset=dataset_tokenized[\"train\"],\n",
    "        eval_dataset=dataset_tokenized[\"test\"],\n",
    "    )\n",
    "    \n",
    "    trainer.add_callback(\n",
    "        GenerateEvalCallback(\n",
    "            prompts=prompts,\n",
    "            accelerator=accelerator,\n",
    "        ))\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "notebook_launcher(hello_world, num_processes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "520117f4-b95b-4efa-be91-31ad14a3e897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> outputs_epoch-1.00.txt <==\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: Hello! I am Open Assistant, a chat-based chat-based chat-based chat-based chat-based chat-based chat-based chat\n",
      "\n",
      "### Human: How much is 2+2? Think step by step\n",
      "### Assistant: 1+2 = 1\n",
      "\n",
      "The average height of a sphere depends on the size of the Earth and the distance between the Earth and the\n",
      "\n",
      "### Human: What is on your mind?\n",
      "### Assistant: I am a computer program that is designed to provide a high-quality, high-quality, high-quality, high-quality, high-quality\n",
      "\n",
      "==> outputs_epoch-2.00.txt <==\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: I am Open Assistant, a chat based chat based chat based on the open source chat. I am a chat based chat based on the open source chat\n",
      "\n",
      "### Human: How much is 2+2? Think step by step\n",
      "### Assistant: 3+2 = 2\n",
      "\n",
      "The average height in a 2 is 2.\n",
      "\n",
      "The average height in a 2 is\n",
      "\n",
      "\n",
      "==> outputs_epoch-3.00.txt <==\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: I am an open source AI language model trained by humans. I am trained on a large corpus of text data, which is generated by a large\n",
      "\n",
      "### Human: How much is 2+2? Think step by step\n",
      "### Assistant: 3 + 2 = 3\n",
      "\n",
      "There are 100 000 000 0000 \n",
      "\n",
      "### Human: What is on your mind?\n",
      "### Assistant: I am an AI language model,\n",
      "\n",
      "==> outputs_epoch-4.00.txt <==\n",
      "### Human: Hello! Who are you? Introduce yourself please\n",
      "### Assistant: Hello! I am Open Assistant, a chat based assistant that can answer questions, help with tasks, and more.### Human: I'm looking\n",
      "\n",
      "### Human: How much is 2+2? Think step by step\n",
      "### Assistant: 2+2 is the average size of the average person. \n",
      "\n",
      "However, this is just a rough estimate and can vary depending on factors\n",
      "\n",
      "### Human: What is on your mind?\n",
      "### Assistant: I am not a sentient AI and I do not have the ability to experience emotions. \n"
     ]
    }
   ],
   "source": [
    "!head outputs_*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
